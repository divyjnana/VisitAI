Firstly, in the text_embeddings file, I have used Sentence Transformers to convert the text to embeddings and store it in Chroma DB. Based on the query, it performs a similarity search and retrieves the top three answers from Chroma. The embeddings are then tokenized into readable form using GPT-2. This code accurately answers questions from a given markdown file. We can also modify the code to answer questions from a single or multiple text PDF files which is done in the file text_embeddings_multiple_pdfs file.
The rag_single_pdf file processes only one PDF file containing both text and images. Using Tesseract and Poppler, I extract the images and text from the PDF. These respective elements are then summarized using GPT-3.5 Turbo for text and GPT-4 for images. The summaries for images by GPT-4 are in the form of captions but are detailed with respect to the document. These summaries are documented with respective metadata, and the documents are stored in FAISS. For the documents stored in FAISS, embeddings are generated using OpenAI embeddings. GPT-4 is used as the LLM which, when asked a query, performs a similarity search in FAISS using OpenAI and retrieves the top three closest text and image answers, including the sources of the retrieved contents. These contents are further summarized and produced as one single clean text answer. Since the retrieved images are in the embeddings form, they are converted using Base64 encoding. This code is quite accurate but uses closed sources.
In the gpt_3_5_combined file, multiple PDFs are processed using some open sources. The text is summarized using GPT-3.5 Turbo and images using CLIP. The image summaries are in the form of numbers, and the documents are created with respective metadata and stored in one single FAISS vector using OpenAI embeddings for all the image and text documents. GPT-4 is used as the LLM. However, when a similarity search is performed, it retrieves accurate text responses but an empty array for images. Therefore, I tried using the same models but different vector stores in the gpt_3_5 file. When stored in different vector stores, it retrieves images but not the relevant ones, since the image summaries were already in a number format there might be a issue for openai embeddings to generate embeddings for it to match the text embeddings in one faiss vector , but when stored in different the embeddings retrived is not accurate because two different vector stores.
The open_source file aims to process multiple PDFs using open sources. I have used pdfplumber to extract images and text (suggested because the previous method took a long time). It uses Sentence Transformers to summarize the text and BLIP to summarize the images, which are in the form of captions. These summaries are documented and stored in FAISS, and I have used CLIP to generate embeddings for both text and image documents. The LLM used is BERT. However, since the LLM-generated embeddings differ from the CLIP-generated embeddings in FAISS, the similarity search is not accurate, and the answers wouldn't be accurate.
Further Approach:
Since in the previous files, CLIP is mainly used only for embeddings, we could use BLIP for image summarization which is in string format similar to GPT-4 summaries but less detailed and open source. Therefore, I would suggest modifying the gpt3_5_combined file to use BLIP for image summaries but modify it such that BLIP provides more detailed captions instead of one-line captions. GPT-3.5 Turbo will be used for text summaries and OpenAI embeddings for both text and image documents. Since the image summaries are in string format instead of the number format by CLIP, this approach will store the embeddings for images as well. GPT-4 will be used for the LLM. The rest of the code and workflow remains the same. This approach might work.
